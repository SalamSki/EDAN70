{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import json\n",
    "datafiles= {\n",
    "  \"E1\" : [''],\n",
    "  \"E2\" : ['a', 'b'],\n",
    "  \"E3\" : [''],\n",
    "  \"E4\" : ['']\n",
    "}\n",
    "datasets = []\n",
    "\n",
    "for i,edition in enumerate(list(datafiles.keys())):\n",
    "  dataset = \"\"\n",
    "  for file in datafiles.get(edition):\n",
    "    with open(f\"./dataset/NF_{edition}{file}.txt\", \"r\", encoding='utf-8') as fr:\n",
    "      dataset += fr.read()\n",
    "      fr.close()\n",
    "  # for trash in re.findall(r\"<h1.*>.*|<h2.*>.*|<h3.*>.*|<a.*>.*|<div.*>.*|<hr>\", dataset):\n",
    "  #   print(trash)\n",
    "  datasets.append(re.sub(r\"\\n{2,}\\s+(?=[a-z])\", \" \", re.sub(r\"&.+;\", \"\", dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tag_dict = []\n",
    "next_chars = 500\n",
    "d = datasets[0]\n",
    "\n",
    "# BUILD POSITIVE \n",
    "for match in re.finditer(r\"((?<=<b>).+<\\/b>)(.*(?<=<b>).+<\\/b>)*\", d):\n",
    "  g1 = match.group(0)\n",
    "  matched_b_tag = re.sub(r\"</b>.*<b>|</b>\",\" \",g1).strip()\n",
    "  end_of_b_tag = match.end()  \n",
    "  surrounding_text_match = re.search(r\"([^<]{1,\"+str(next_chars)+r\"})(?=<|$)\", d[end_of_b_tag:end_of_b_tag+next_chars])\n",
    "  surrounding_text = surrounding_text_match.group(0) if surrounding_text_match else \"\"\n",
    "\n",
    "  short_def = re.sub(r\"\\s+\", \" \", surrounding_text).strip()\n",
    "  if len(short_def) > 0:\n",
    "    b_tag_dict.append([f\"{matched_b_tag} {short_def}\", matched_b_tag])\n",
    "\n",
    "\n",
    "# BUILD NEGATIVE\n",
    "for match in re.finditer(r\"(\\n\\n[^<]{10,500})(?=\\n|$|<)\", d):\n",
    "  g = match.group(0)\n",
    "  matched_text = re.sub(r\"\\s+\", \" \", g).strip()\n",
    "  b_tag_dict.append([matched_text, \"\"])\n",
    "\n",
    "print(len(b_tag_dict))\n",
    "with open(\"NF_E1_B.json\", \"w\") as b_json:\n",
    "  json.dump(b_tag_dict, b_json, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sequential_page_patterns(text, remove=False):\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    patterns_found = []\n",
    "    line_indices_to_remove = set()\n",
    "\n",
    "\n",
    "    for i in range(len(lines) - 2):\n",
    "        try:\n",
    "            first_num = int(lines[i])\n",
    "            sec_num = int(lines[i + 4])\n",
    "            if sec_num == first_num + 1:\n",
    "                middle_text = lines[i + 2]\n",
    "                pattern = f\"{first_num}\\n\\n{middle_text}\\n\\n{sec_num}\"\n",
    "                patterns_found.append(pattern)\n",
    "                line_indices_to_remove.update([i-1, i, i+1, i + 2, i + 3, i + 4, i + 5]) \n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    print(f\"Found {len(patterns_found)} patterns:\\n\")\n",
    "    for i, pattern in enumerate(patterns_found, 1):\n",
    "        print(f\"Pattern #{i}:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(pattern)\n",
    "        print(\"-\" * 40)\n",
    "        print()\n",
    "    #print (line_indices_to_remove)\n",
    "    if remove:\n",
    "        new_lines = [line for idx, line in enumerate(lines) if idx not in line_indices_to_remove]\n",
    "        while new_lines and not new_lines[-1].strip():\n",
    "            new_lines.pop()\n",
    "        return '\\n'.join(new_lines)\n",
    "    return text\n",
    "    #return patterns_found\n",
    "\n",
    "\n",
    "def process_file(input_file_path, remove=False):\n",
    "    try:\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        result = find_sequential_page_patterns(text, remove)\n",
    "\n",
    "        if remove:\n",
    "            with open(\"dataset/NF_E4.txt\", 'w', encoding='utf-8') as file:\n",
    "                file.write(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_file(\"dataset/NF_E4.txt\", remove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
