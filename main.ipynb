{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "Imports and define names of datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T01:02:45.075847Z",
     "iopub.status.busy": "2024-11-16T01:02:45.075725Z",
     "iopub.status.idle": "2024-11-16T01:03:04.070511Z",
     "shell.execute_reply": "2024-11-16T01:03:04.069918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  \n",
    "from typing import List,Tuple\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "datafiles= {\n",
    "  \"E1\" : [''],\n",
    "  \"E2\" : ['a', 'b'],\n",
    "  \"E3\" : [''],\n",
    "  \"E4\" : ['']\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that extracts headwords out of \\<b\\> tags to build a headword dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T01:03:04.072450Z",
     "iopub.status.busy": "2024-11-16T01:03:04.072169Z",
     "iopub.status.idle": "2024-11-16T01:03:04.076453Z",
     "shell.execute_reply": "2024-11-16T01:03:04.076091Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_b_tag_dataset(datastring, next_chars = 500, verbose=False):\n",
    "  b_tag_dict = []\n",
    "\n",
    "  # BUILD POSITIVE \n",
    "  for match in tqdm(re.finditer(r\"((?<=<b>).+<\\/b>)(.*(?<=<b>).+<\\/b>)*\", datastring), disable=(not verbose)):\n",
    "    g1 = match.group(0)\n",
    "    matched_b_tag = re.sub(r\"</b>.*<b>|</b>\",\" \",g1).strip()\n",
    "    end_of_b_tag = match.end()  \n",
    "    \n",
    "    surrounding_text_match = re.search(r\"([^<]{1,\"+str(next_chars)+r\"})(?=<|$)\", datastring[end_of_b_tag:end_of_b_tag+next_chars])\n",
    "    surrounding_text = surrounding_text_match.group(0) if surrounding_text_match else \"\"\n",
    "\n",
    "    short_def = re.sub(r\"\\s+\", \" \", surrounding_text).strip()\n",
    "    if len(short_def) > 0:\n",
    "      b_tag_dict.append([f\"{matched_b_tag} {short_def}\", matched_b_tag])\n",
    "\n",
    "  # BUILD NEGATIVE\n",
    "  for match in tqdm(re.finditer(r\"(\\n\\n\\p{Upper}[^<]{10,500})(?=\\n|$|<)\", datastring), disable=(not verbose)):\n",
    "    g = match.group(0)\n",
    "    matched_text = re.sub(r\"\\s+\", \" \", g).strip()\n",
    "    b_tag_dict.append([matched_text, \"\"])\n",
    "\n",
    "  return b_tag_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the headword datasets for the first and second editions (E1 \\& E2) where for each entry there is:\n",
    "  - Feature: A paragraph or piece of text that starts with a headword, followed by up to <i>next_chars</i> number of characters, default is 500.\n",
    "  - label: The headword at the beginning of the corresponding feature, empty string if feature wasn't a <i>\"headword\"</i> paragraph.\n",
    "\n",
    "Save results to json files:\n",
    "```json\n",
    "  [\"Lund, uppstad i Malmöhus län...beskaffenhet. I all\", \"Lund,\"]\n",
    "  [\"betjenade sig af rapporter från...till privatlifvet\", \"\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T01:03:04.077819Z",
     "iopub.status.busy": "2024-11-16T01:03:04.077689Z",
     "iopub.status.idle": "2024-11-16T01:03:04.079708Z",
     "shell.execute_reply": "2024-11-16T01:03:04.079366Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_json_headword_set():\n",
    "  for edition in ['E1', 'E2']:\n",
    "\n",
    "    dataset = \"\"\n",
    "    for file in datafiles.get(edition):\n",
    "      with open(f\"./dataset/NF_{edition}{file}.txt\", \"r\", encoding='utf-8') as fr:\n",
    "        dataset += fr.read()\n",
    "        fr.close()\n",
    "        \n",
    "    b_tag_dict = build_b_tag_dataset(dataset, verbose=True)\n",
    "    print(f\"{edition} has {len(b_tag_dict):,} entries\")\n",
    "\n",
    "    with open(f\"./dataset/NF_{edition}_B.json\", \"w\") as b_json:\n",
    "      json.dump(b_tag_dict, b_json, indent=2, ensure_ascii=False)\n",
    "  del edition, dataset, file, fr, b_tag_dict, b_json\n",
    "# build_json_headword_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T01:03:04.526659Z",
     "iopub.status.busy": "2024-11-16T01:03:04.526523Z",
     "iopub.status.idle": "2024-11-16T01:05:47.982547Z",
     "shell.execute_reply": "2024-11-16T01:05:47.981956Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:03<00:00, 1515.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([303448, 100]), torch.Size([303448, 100]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased\")\n",
    "BUILD_HEADWORD = False\n",
    "\n",
    "def process_data(sentence, headword):\n",
    "    encoded_sentence = tokenizer(\n",
    "        sentence,\n",
    "        add_special_tokens=True, \n",
    "        padding='max_length',   \n",
    "        max_length=100,        \n",
    "        truncation=True,       \n",
    "        return_tensors='pt'  \n",
    "    )\n",
    "    encoded_headword = tokenizer(\n",
    "        headword,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        max_length=20,           \n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_sentence['input_ids'][0], encoded_headword['input_ids'][0]\n",
    "\n",
    "def extract_features_labels(dataset) -> Tuple[List, List]:\n",
    "    x = []\n",
    "    y = []\n",
    "    for entry in tqdm(dataset):\n",
    "      sentence, headword = process_data(entry[0], entry[1])\n",
    "      x.append(sentence)\n",
    "\n",
    "      min_len = min(len(sentence), len(headword))\n",
    "      headword_mask = np.where((sentence[:min_len] > 4) & (sentence[:min_len] == headword[:min_len]), 1, 0)\n",
    "      headword_mask = np.pad(headword_mask, (0, len(sentence) - min_len), 'constant')\n",
    "      \n",
    "      y.append(torch.tensor(headword_mask))\n",
    "    return torch.stack(x).to(device), torch.stack(y).to(device)\n",
    "\n",
    "def build_headword_dataset():\n",
    "  def load_headword_json():\n",
    "    out = []\n",
    "    for edition in ['E1', 'E2']:\n",
    "        with open(f\"./dataset/NF_{edition}_B.json\", \"r\", encoding='utf-8') as b_json:\n",
    "          out += json.load(b_json)\n",
    "          b_json.close()\n",
    "    return out\n",
    "  dataset = load_headword_json()\n",
    "  random.seed(123)\n",
    "  random.shuffle(dataset)\n",
    "  dataset = dataset[5000:]\n",
    "  temp_X, temp_y = extract_features_labels(dataset)\n",
    "  torch.save((temp_X,temp_y),'./dataset/headword_dataset.pth')\n",
    "  return temp_X, temp_y\n",
    "\n",
    "# Load in manually annotated test set. \n",
    "with open(\"./dataset/NF_test_set_12_annotated.json\", \"r\", encoding='utf-8') as annotated_test:\n",
    "  test_set = json.load(annotated_test)\n",
    "  annotated_test.close()\n",
    "X_test, y_test = extract_features_labels(test_set) # <-- Use this to comapre different models. \n",
    "\n",
    "# Either build pytorch dataset out of json, or load in saved pth file.\n",
    "if BUILD_HEADWORD:\n",
    "  X, y = build_headword_dataset()            \n",
    "else:\n",
    "  X, y = torch.load('./dataset/headword_dataset.pth', weights_only=True)\n",
    "X.shape, y.shape # <-- Use this to train our model or fine-tune a model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: \n",
      "Bondesen, Ingvor, dansk författare, f. 1844, var 186485 skollärare på Fyn, blef därefter anställd i Köpenhamns skolväsen och 1892 skolinspektör vid en af skolorna därstädes. Han började 1877 under märket Henning Fox att skrifva historiska romaner från den äldre medeltiden, Styrismanden og hans brud ( 1877 ) och Kongsbrydens fostersön ( 1878 ). Senare följde berättelser med ämnen från adelsväldets tid, Rettergang og skriftegang\n",
      "\n",
      "Tokenized Sentence: \n",
      "[2, 17431, 436, 19, 1613, 8143, 19, 5908, 4567, 19, 13, 7, 21978, 49841, 19, 96, 42581, 5727, 16964, 4759, 68, 48349, 19, 21829, 49808, 3386, 7410, 31, 32754, 1005, 28343, 36, 35907, 1005, 17969, 252, 59, 4815, 13349, 256, 23055, 7, 371, 1649, 42456, 244, 19997, 15860, 13469, 48, 1147, 49808, 384, 8521, 15797, 146, 97, 2332, 21045, 19, 15307, 2894, 963, 5196, 699, 14503, 177, 42456, 171, 36, 7954, 9213, 389, 148, 11989, 21345, 177, 40999, 171, 7, 9475, 5622, 12631, 66, 7114, 146, 23410, 746, 1730, 353, 290, 19, 17187, 95, 19612, 5196, 4653, 24640, 49802, 3]\n",
      "\n",
      "Target Mask: \n",
      "[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Target Headword: \n",
      "Bondesen,\n"
     ]
    }
   ],
   "source": [
    "example_index = 1\n",
    "input = X[example_index]\n",
    "mask = y[example_index]\n",
    "headword = X[example_index][torch.flatten(torch.nonzero(y[example_index]))]\n",
    "print(f\"Input Sentence: \\n{tokenizer.decode(input, skip_special_tokens=True)}\\n\")\n",
    "print(f\"Tokenized Sentence: \\n{input.tolist()}\\n\")\n",
    "print(f\"Target Mask: \\n{mask.tolist()}\\n\")\n",
    "print(f\"Target Headword: \\n{tokenizer.decode(headword, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T01:05:47.986360Z",
     "iopub.status.busy": "2024-11-16T01:05:47.986218Z",
     "iopub.status.idle": "2024-11-16T01:05:48.146526Z",
     "shell.execute_reply": "2024-11-16T01:05:48.145965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50325\n",
      "Max input ID: 49968\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20) # <-- Use to train our model or fine-tune a model.\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Max input ID:\", torch.max(X_train).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T01:05:53.055633Z",
     "iopub.status.busy": "2024-11-16T01:05:53.055402Z",
     "iopub.status.idle": "2024-11-16T01:05:53.058296Z",
     "shell.execute_reply": "2024-11-16T01:05:53.057940Z"
    }
   },
   "outputs": [],
   "source": [
    "class HeadwordExtractor():\n",
    "  def __init__(self, saved_model):\n",
    "    self.embedding_dim = 128\n",
    "    self.hidden_dim = 128\n",
    "    self.batch_size = 32\n",
    "    self.num_epochs = 5\n",
    "    self.learning_rate = 0.001\n",
    "    self.saved_model = saved_model\n",
    "    \n",
    "    self.train_loader = DataLoader(TensorDataset(X_train.long(), y_train.long()), batch_size=self.batch_size, shuffle=True)\n",
    "    self.val_loader = DataLoader(TensorDataset(X_val.long(), y_val.long()), batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    self.model = self.EncoderLSTM(tokenizer.vocab_size, self.embedding_dim, self.hidden_dim, nbr_classes=2, num_layers=1,bidi_lstm=True).to(device)\n",
    "\n",
    "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    print(self.model)\n",
    "\n",
    "  class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, nbr_classes, num_layers=1, bidi_lstm=False):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=tokenizer.pad_token_id)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidi_lstm)\n",
    "        if not bidi_lstm:\n",
    "            self.fc = nn.Linear(hidden_dim, nbr_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(2*hidden_dim, nbr_classes)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)\n",
    "        encoder_out, _ = self.encoder(embeds)\n",
    "        encoder_out = nn.functional.relu(encoder_out)\n",
    "        logits = self.fc(encoder_out)\n",
    "        return logits\n",
    "\n",
    "  def train_extractor(self):\n",
    "    history=[]\n",
    "    for epoch in range(self.num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{self.num_epochs}:\")\n",
    "      \n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "        temp = 0\n",
    "        for input_batch, target_batch in tqdm(self.train_loader, desc = \"Training\"):\n",
    "            outputs = self.model(input_batch)\n",
    "\n",
    "            loss = self.criterion(outputs.view(-1,outputs.shape[-1]), target_batch.view(-1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_accuracy += (torch.sum(outputs.view(-1,outputs.shape[-1]).argmax(dim=1) == target_batch.view(-1))/target_batch.view(-1).shape[0]).item()\n",
    "        avg_train_loss = train_loss / len(self.train_loader)\n",
    "        avg_train_acc = train_accuracy / len(self.train_loader)\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_acc:.4f}\")\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            val_accuracy = 0\n",
    "            for input_batch, target_batch in tqdm(self.val_loader, desc = \"Validation\"):\n",
    "              outputs = self.model(input_batch)\n",
    "\n",
    "              loss = self.criterion(outputs.view(-1,outputs.shape[-1]), target_batch.view(-1))\n",
    "              val_loss += loss.item()\n",
    "              val_accuracy += (torch.sum(outputs.view(-1,outputs.shape[-1]).argmax(dim=1) == target_batch.view(-1))/target_batch.view(-1).shape[0]).item()\n",
    "        avg_val_loss = val_loss/len(self.val_loader)\n",
    "        avg_val_acc = val_accuracy / len(self.val_loader)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_acc:.4f}\\n\")\n",
    "\n",
    "        history.append((avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc))\n",
    "    self.__plot_metrics(history)\n",
    "\n",
    "  def __plot_metrics(self, history):\n",
    "    train_loss, train_acc, val_loss, val_acc = tuple(zip(*history))\n",
    "    epochs = range(1, len(history) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_acc, marker='o', linestyle='-', label='Train Accuracy')\n",
    "    plt.plot(epochs, val_acc, marker='o', linestyle='-', label='Validation Accuracy')\n",
    "    plt.title('Validation vs Train Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, train_loss, marker='o', linestyle='-', label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, marker='o', linestyle='-', label='Validation Loss')\n",
    "    plt.title('Validation vs Train Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "  def predict(self, encoded_input):\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "      output_mask = self.model(encoded_input).argmax(dim=-1).cpu()\n",
    "      \n",
    "    for entry_index, token_index in enumerate(output_mask.argmax(1)):\n",
    "      while_index = token_index\n",
    "      while output_mask[entry_index, while_index] == 1 and while_index > 0 and tokenizer.decode(encoded_input[entry_index, while_index]).startswith(\"##\"):\n",
    "        output_mask[entry_index, while_index-1] = 1\n",
    "        while_index-=1\n",
    "\n",
    "    return output_mask\n",
    "  \n",
    "  def load_model(self):\n",
    "     self.model.load_state_dict(torch.load(self.saved_model, weights_only=True))\n",
    "     self.model.eval()\n",
    "  \n",
    "  def save_model(self):\n",
    "     torch.save(self.model.state_dict(), self.saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train or load model from saved file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLSTM(\n",
      "  (embeddings): Embedding(50325, 128, padding_idx=0)\n",
      "  (encoder): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "SAVED_MODEL = \"headword_extractor.pth\"\n",
    "TRAIN_NEW_MODEL = False\n",
    "\n",
    "extractor = HeadwordExtractor(saved_model=SAVED_MODEL)\n",
    "\n",
    "if TRAIN_NEW_MODEL:\n",
    "  extractor.train_extractor()\n",
    "  extractor.save_model()\n",
    "else:\n",
    "  extractor.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.63%\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    484419\n",
      "           1       0.96      0.92      0.94     15581\n",
      "\n",
      "    accuracy                           1.00    500000\n",
      "   macro avg       0.98      0.96      0.97    500000\n",
      "weighted avg       1.00      1.00      1.00    500000\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test_predictions = extractor.predict(X_test).view(-1)\n",
    "y_test = y_test.view(-1).cpu()\n",
    "\n",
    "print(f\"Test Accuracy: {(accuracy_score(y_test, y_test_predictions)*100):.2f}%\")\n",
    "print(f\"Classification Report: \\n{classification_report(y_test, y_test_predictions)}\")\n",
    "cm = confusion_matrix(y_test, y_test_predictions, normalize='pred')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=[0,1])\n",
    "cmd.plot()\n",
    "print(\"Confusion Matrix:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T04:02:51.484972Z",
     "iopub.status.busy": "2024-11-16T04:02:51.484820Z",
     "iopub.status.idle": "2024-11-16T04:02:51.586294Z",
     "shell.execute_reply": "2024-11-16T04:02:51.585934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted headword: Gustav Vasa,\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"Gustav Vasa, ursprungligen Gustav Eriksson,[2] enligt flera källor född 12 maj 1496, död 29 september 1560 på Tre Kronor i Stockholm.[3] var kung av Sverige 1523–1560 och riksföreståndare 1521–1523, under det pågående befrielsekriget. Hans makttillträde, inlett som ett uppror mot unionskungen Kristian II efter Stockholms blodbad, innebar slutet för\"\n",
    "encoded_input = tokenizer(input_sentence, return_tensors=\"pt\", padding = \"max_length\", max_length = 100, truncation = True)['input_ids'].to(device)\n",
    "\n",
    "output_mask = extractor.predict(encoded_input).view(-1)\n",
    "headword = encoded_input.cpu().view(-1)[torch.flatten(torch.nonzero(output_mask))]\n",
    "print(\"Predicted headword:\", tokenizer.decode(headword, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_editions():\n",
    "  tokenized_editions = []\n",
    "  for edition in datafiles.keys():\n",
    "    edition_data = \"\"\n",
    "    for file in datafiles.get(edition):\n",
    "      with open(f\"./dataset/NF_{edition}{file}.txt\", \"r\", encoding='utf-8') as fr:\n",
    "        edition_data += fr.read()\n",
    "        fr.close()\n",
    "    edition_data = re.sub(r\"<b>|</b>\", \"\", edition_data)\n",
    "    \n",
    "    splitted_paragraphs = re.split(r\"\\n\\n\", edition_data)\n",
    "    filterd_paragraphs = filter(lambda p: len(p) >= 10, splitted_paragraphs)\n",
    "    truncated_paragraphs = map(lambda p: p[:500] if len(p) > 500 else p, filterd_paragraphs)\n",
    "    \n",
    "    paragraphs = torch.stack(\n",
    "      [tokenizer(\n",
    "          p,\n",
    "          add_special_tokens=True, \n",
    "          padding='max_length',   \n",
    "          max_length=100,        \n",
    "          truncation=True,       \n",
    "          return_tensors='pt'  \n",
    "        )['input_ids'][0] \n",
    "        for p in tqdm(list(truncated_paragraphs))\n",
    "      ]).to(device)\n",
    "    tokenized_editions.append(paragraphs)\n",
    "  torch.save(tokenized_editions, './dataset/tokenized_editions.pth')\n",
    "  return tokenized_editions\n",
    "\n",
    "SHOULD_TOKENIZE_EDITIONS = False\n",
    "if SHOULD_TOKENIZE_EDITIONS:\n",
    "  editions = tokenize_editions()\n",
    "else:\n",
    "  editions = torch.load('./dataset/tokenized_editions.pth', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117629, 178449, 25005, 84167)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_editions():\n",
    "  db = {}\n",
    "  for ei, edition in enumerate(editions):\n",
    "    # Predict editions by batches:\n",
    "    edition_loader = DataLoader(edition, batch_size=19000, shuffle=False)\n",
    "    edition_predictions = torch.empty((0,100)).to(device)\n",
    "    for batch in tqdm(edition_loader, desc=f\"Predicting Edtion E{ei+1}\"):\n",
    "      batch_prediction = extractor.predict(batch).to(device)\n",
    "      edition_predictions = torch.cat((edition_predictions, batch_prediction))\n",
    "\n",
    "    # Filter away non-headword predictions:\n",
    "    predicted_input = edition[torch.unique(torch.nonzero(edition_predictions)[:, 0])]\n",
    "    predicted_masks = edition_predictions[torch.unique(torch.nonzero(edition_predictions)[:, 0])]\n",
    "    predicted_entires = []\n",
    "    entry_cnt = 0\n",
    "    for input, mask in tqdm(list(zip(predicted_input,predicted_masks)), desc=f\"Decoding   Edtion E{ei+1}\"):\n",
    "      decoded_headword = tokenizer.decode(input[mask.nonzero().flatten()], skip_special_tokens=True)\n",
    "      decoded_headword = re.sub(r\",\", \"\", decoded_headword)\n",
    "      if decoded_headword != \"\":\n",
    "        decoded_input = tokenizer.decode(input, skip_special_tokens=True)\n",
    "\n",
    "        if not re.search(r\"^Bild [\\diI]+\", decoded_input):\n",
    "          entry_cnt+=1\n",
    "          predicted_entires.append({\"entry_id\": f\"E{ei+1}_{entry_cnt}\", \"headword\": decoded_headword, \"definition\": decoded_input})\n",
    "          \n",
    "    db[f\"E{ei+1}\"] = predicted_entires\n",
    "\n",
    "  with open(\"./extracted_entries.json\", \"w\") as entry_json:\n",
    "    json.dump(db, entry_json, indent=2, ensure_ascii=False)\n",
    "    entry_json.close()\n",
    "\n",
    "def load_in_db():\n",
    "  try:\n",
    "    with open(\"./extracted_entries.json\", \"r\", encoding='utf-8') as entry_json_r:\n",
    "      db = json.load(entry_json_r)\n",
    "      entry_json_r.close()\n",
    "  except:\n",
    "    db = {}\n",
    "  return db\n",
    "\n",
    "# predict_editions()\n",
    "db = load_in_db()\n",
    "len(db['E1']), len(db['E2']), len(db['E3']), len(db['E4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 10/10 [00:00<00:00, 64.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 72.46it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 63.70it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 71.94it/s]\n"
     ]
    }
   ],
   "source": [
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"KB/bert-base-swedish-cased-ner\").to(device)\n",
    "\n",
    "def classify_entity(sentence, headword):\n",
    "  \"\"\"\n",
    "  Returns\n",
    "    0 --> Other\\n\n",
    "    1 --> Location\\n\n",
    "    2 --> Person\n",
    "  \"\"\"\n",
    "  encoded_sentence = tokenizer(sentence,add_special_tokens=True,padding='max_length',max_length=100,truncation=True,return_tensors='pt').to(device)\n",
    "  encoded_headword = tokenizer(headword,add_special_tokens=True,padding='max_length',max_length=20,truncation=True,return_tensors='pt')['input_ids'][0]\n",
    "\n",
    "  with torch.no_grad():\n",
    "    encoded_outputs = torch.argmax(ner_model(**encoded_sentence).logits, dim=2)\n",
    "\n",
    "  labels = encoded_outputs.flatten()[(encoded_headword > 4).nonzero()].flatten()\n",
    "  if 7 in labels and 9 in labels:\n",
    "    return 0\n",
    "  elif 7 in labels:\n",
    "    return 1\n",
    "  elif 9 in labels:\n",
    "    return 2\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "for e in db:\n",
    "  entries = db[e]\n",
    "\n",
    "  for entry in tqdm(entries[:10]):\n",
    "    ner = classify_entity(entry['definition'], entry['headword'])\n",
    "    entry['type'] = ner\n",
    "  \n",
    "with open(\"./ner_entires.json\", \"w\") as ner_json:\n",
    "  json.dump(db, ner_json, indent=2, ensure_ascii=False)\n",
    "  ner_json.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
