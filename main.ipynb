{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "Imports and define names of datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List,Tuple\n",
    "from tqdm import tqdm  \n",
    "import regex as re\n",
    "import random\n",
    "import json\n",
    "datafiles= {\n",
    "  \"E1\" : [''],\n",
    "  \"E2\" : ['a', 'b'],\n",
    "  \"E3\" : [''],\n",
    "  \"E4\" : ['']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that extracts headwords out of \\<b\\> tags to build a headword dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_b_tag_dataset(datastring, next_chars = 500, verbose=False):\n",
    "  b_tag_dict = []\n",
    "\n",
    "  # BUILD POSITIVE \n",
    "  for match in tqdm(re.finditer(r\"((?<=<b>).+<\\/b>)(.*(?<=<b>).+<\\/b>)*\", datastring), disable=(not verbose)):\n",
    "    g1 = match.group(0)\n",
    "    matched_b_tag = re.sub(r\"</b>.*<b>|</b>\",\" \",g1).strip()\n",
    "    end_of_b_tag = match.end()  \n",
    "    \n",
    "    surrounding_text_match = re.search(r\"([^<]{1,\"+str(next_chars)+r\"})(?=<|$)\", datastring[end_of_b_tag:end_of_b_tag+next_chars])\n",
    "    surrounding_text = surrounding_text_match.group(0) if surrounding_text_match else \"\"\n",
    "\n",
    "    short_def = re.sub(r\"\\s+\", \" \", surrounding_text).strip()\n",
    "    if len(short_def) > 0:\n",
    "      b_tag_dict.append([f\"{matched_b_tag} {short_def}\", matched_b_tag])\n",
    "\n",
    "  # BUILD NEGATIVE\n",
    "  for match in tqdm(re.finditer(r\"(\\n\\n\\p{Upper}[^<]{10,500})(?=\\n|$|<)\", datastring), disable=(not verbose)):\n",
    "    g = match.group(0)\n",
    "    matched_text = re.sub(r\"\\s+\", \" \", g).strip()\n",
    "    b_tag_dict.append([matched_text, \"<NO_HEADWORD>\"])\n",
    "\n",
    "  return b_tag_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the headword datasets for the first and second editions (E1 \\& E2) where for each entry there is:\n",
    "  - Feature: A paragraph or piece of text that starts with a headword, followed by up to <i>next_chars</i> number of characters, default is 500.\n",
    "  - label: The headword at the beginning of the corresponding feature, empty string if feature wasn't a <i>\"headword\"</i> paragraph.\n",
    "\n",
    "Save results to json files:\n",
    "```json\n",
    "  [\"Lund, uppstad i Malmöhus län...beskaffenhet. I all\", \"Lund,\"]\n",
    "  [\"betjenade sig af rapporter från...till privatlifvet\", \"\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114857it [00:18, 6239.93it/s] \n",
      "11920it [00:00, 18291.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1 has 126,690 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "132408it [00:26, 4955.58it/s]\n",
      "49493it [00:02, 18965.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2 has 181,758 entries\n"
     ]
    }
   ],
   "source": [
    "for i,edition in enumerate(['E1', 'E2']):\n",
    "\n",
    "  dataset = \"\"\n",
    "  for file in datafiles.get(edition):\n",
    "    with open(f\"./dataset/NF_{edition}{file}.txt\", \"r\", encoding='utf-8') as fr:\n",
    "      dataset += fr.read()\n",
    "      fr.close()\n",
    "      \n",
    "  b_tag_dict = build_b_tag_dataset(dataset, verbose=True)\n",
    "  print(f\"{edition} has {len(b_tag_dict):,} entries\")\n",
    "\n",
    "  with open(f\"./dataset/NF_{edition}_B.json\", \"w\") as b_json:\n",
    "    json.dump(b_tag_dict, b_json, indent=2, ensure_ascii=False)\n",
    "del i, edition, dataset, file, fr, b_tag_dict, b_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i, edition in enumerate(['E1', 'E2']):\n",
    "  for file in datafiles.get(edition):\n",
    "    with open(f\"./dataset/NF_{edition}_B.json\", \"r\", encoding='utf-8') as b_json:\n",
    "      dataset += json.load(b_json)\n",
    "      b_json.close()\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased\")\n",
    "_ = tokenizer.add_tokens([\"<NO_HEADWORD>\"])\n",
    "\n",
    "def process_data(sentence, headword):\n",
    "    encoded_sentence = tokenizer(\n",
    "        sentence,\n",
    "        add_special_tokens=True, # Add [CLS] and [SEP] tokens\n",
    "        padding='max_length',   # Pad to a maximum length\n",
    "        max_length=50,        # Choose an appropriate max length\n",
    "        truncation=True,        # Truncate if longer than max length\n",
    "        return_tensors='pt'   # Return PyTorch tensors\n",
    "    )\n",
    "    # Encode the headword\n",
    "    encoded_headword = tokenizer(\n",
    "        headword,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        max_length=32,           # Choose a suitable max length for headwords\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_sentence['input_ids'][0], encoded_headword['input_ids'][0]\n",
    "\n",
    "def extract_features_labels(dataset) -> Tuple[List, List]:\n",
    "    x = []\n",
    "    y = []\n",
    "    for entry in tqdm(dataset):\n",
    "      s, h = process_data(entry[0], entry[1])\n",
    "      x.append(s)\n",
    "      y.append(h)\n",
    "    return x,y\n",
    "\n",
    "dataset = dataset[:int(0.2*len(dataset))]\n",
    "X, y = extract_features_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[    2,   335,    76, 11691, 49796,  5548, 40176,  8847,   264, 33282,\n",
       "          33628,    42,  6749,  2879,  2346,    19,   256,   102, 33013, 49795,\n",
       "           1058, 26707,  6749,  2213,  2650,   252, 11050,    19,  2879,  2346,\n",
       "             19,   146,    21, 10996,   308, 39949,  4815,  2348,  2112,   178,\n",
       "          42812, 49795,  1376, 49808, 13957,  4261,   390,  1783,   290,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]])},\n",
       " {'input_ids': tensor([[    2,  2876,   379,    19, 11729,     7,  1683,  2609,  3114,  8051,\n",
       "              7,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]])},\n",
       " {'input_ids': tensor([[    2,  8361, 16044, 49800, 19527, 49800,   495, 19885,  7830, 49811,\n",
       "            765,   399, 49811,   496,    19,   836,     7,  8361, 16044, 49800,\n",
       "          21473,    19, 14015, 18984,    36,  5760,    19,    13,     7, 19140,\n",
       "          49823,    31,  2871,    19,    15,     7, 23650, 49846,    31, 26359,\n",
       "           6291,    19,    96,    97,  8656,   328,   758, 29186, 49796,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]])},\n",
       " {'input_ids': tensor([[    2,   106,     7, 10882,     7,   135,  3562,    19,    67,   244,\n",
       "          10638,    52,    36,  8778,    52, 14530, 16282, 49808, 31133,   778,\n",
       "            394, 20838, 49796,  4815,   829,  5051,  7315, 49794,    19,  2752,\n",
       "            140,    97,  4402, 13466,  4815, 22329, 49800, 10020,  2841,  5810,\n",
       "          49796,   135,  4560,  1235, 49797,   196,   269, 49797,   177,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]])},\n",
       " {'input_ids': tensor([[    2,  3109, 49903, 16808,   495,   999,   329, 49798,   496,    19,\n",
       "           1052,   177,   878, 19792,   562,   171,    19,  7043, 17246,  6078,\n",
       "             19, 34803, 19639, 49846,    19,   365,   243, 18268, 20450,   364,\n",
       "            244,  4280, 23323, 49432,   177,  8337, 49823,    52,  1831,   171,\n",
       "              7,     3,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]])},\n",
       " {'input_ids': tensor([[    2, 14155,  1178,   177, 34940,  1178,   171,     7,  1683,  9410,\n",
       "           8443,   667,     7,     3,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]])},\n",
       " {'input_ids': tensor([[    2, 28355,  3786, 28930,  1667, 10921,     7,  1683,  4603,  4088,\n",
       "              7, 28355, 23967,   177,    53,   126,    17,   333,   171,    19,\n",
       "           4261,   400, 49808,   170,  7295, 49795,  1849, 28325, 23967,   177,\n",
       "            653, 28355,  2039,   171,    19,    96, 11066,   869, 49795,   653,\n",
       "          28355,  2039,    66,    59,  6268,  4815,    61,    19,   487,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]])},\n",
       " {'input_ids': tensor([[    2,  3050, 29744,    19,    83,     7,   150,     7,    19,    96,\n",
       "           2632,  4815,    46,     7,   296,   177,  1906,    46,     7,   319,\n",
       "            171,     7,     3,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]])},\n",
       " {'input_ids': tensor([[    2,    99,   205,   343,  6936,    31,    59,  1077, 39999,   440,\n",
       "           6753,    36, 14235,    19,    67, 16787,    31,   134,  1690,  1074,\n",
       "          12997, 20297, 26138,     7,  1932,    19,    67,  6749, 13539,    31,\n",
       "          15501, 13197,   480,    48, 12092,  4815, 11547,  3170,    19,  6749,\n",
       "           2308,  6936,    66,  5050,  4815,  2112,  9585,    89, 40375,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1]])},\n",
       " {'input_ids': tensor([[    2,  2042, 21203, 18697,     7,  1683,  2042, 21203,     7,     3,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]])}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
