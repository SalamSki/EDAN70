{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "Imports and define names of datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T01:02:45.075847Z",
     "iopub.status.busy": "2024-11-16T01:02:45.075725Z",
     "iopub.status.idle": "2024-11-16T01:03:04.070511Z",
     "shell.execute_reply": "2024-11-16T01:03:04.069918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  \n",
    "from typing import List,Tuple\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "datafiles= {\n",
    "  \"E1\" : [''],\n",
    "  \"E2\" : ['a', 'b'],\n",
    "  \"E3\" : [''],\n",
    "  \"E4\" : ['']\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that extracts headwords out of \\<b\\> tags to build a headword dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T01:03:04.072450Z",
     "iopub.status.busy": "2024-11-16T01:03:04.072169Z",
     "iopub.status.idle": "2024-11-16T01:03:04.076453Z",
     "shell.execute_reply": "2024-11-16T01:03:04.076091Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_b_tag_dataset(datastring, next_chars = 500, verbose=False):\n",
    "  b_tag_dict = []\n",
    "\n",
    "  # BUILD POSITIVE \n",
    "  for match in tqdm(re.finditer(r\"((?<=<b>).+<\\/b>)(.*(?<=<b>).+<\\/b>)*\", datastring), disable=(not verbose)):\n",
    "    g1 = match.group(0)\n",
    "    matched_b_tag = re.sub(r\"</b>.*<b>|</b>\",\" \",g1).strip()\n",
    "    end_of_b_tag = match.end()  \n",
    "    \n",
    "    surrounding_text_match = re.search(r\"([^<]{1,\"+str(next_chars)+r\"})(?=<|$)\", datastring[end_of_b_tag:end_of_b_tag+next_chars])\n",
    "    surrounding_text = surrounding_text_match.group(0) if surrounding_text_match else \"\"\n",
    "\n",
    "    short_def = re.sub(r\"\\s+\", \" \", surrounding_text).strip()\n",
    "    if len(short_def) > 0:\n",
    "      b_tag_dict.append([f\"{matched_b_tag} {short_def}\", matched_b_tag])\n",
    "\n",
    "  # BUILD NEGATIVE\n",
    "  for match in tqdm(re.finditer(r\"(\\n\\n\\p{Upper}[^<]{10,500})(?=\\n|$|<)\", datastring), disable=(not verbose)):\n",
    "    g = match.group(0)\n",
    "    matched_text = re.sub(r\"\\s+\", \" \", g).strip()\n",
    "    b_tag_dict.append([matched_text, \"\"])\n",
    "\n",
    "  return b_tag_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the headword datasets for the first and second editions (E1 \\& E2) where for each entry there is:\n",
    "  - Feature: A paragraph or piece of text that starts with a headword, followed by up to <i>next_chars</i> number of characters, default is 500.\n",
    "  - label: The headword at the beginning of the corresponding feature, empty string if feature wasn't a <i>\"headword\"</i> paragraph.\n",
    "\n",
    "Save results to json files:\n",
    "```json\n",
    "  [\"Lund, uppstad i Malmöhus län...beskaffenhet. I all\", \"Lund,\"]\n",
    "  [\"betjenade sig af rapporter från...till privatlifvet\", \"\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T01:03:04.077819Z",
     "iopub.status.busy": "2024-11-16T01:03:04.077689Z",
     "iopub.status.idle": "2024-11-16T01:03:04.079708Z",
     "shell.execute_reply": "2024-11-16T01:03:04.079366Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_json_headword_set():\n",
    "  for i,edition in enumerate(['E1', 'E2']):\n",
    "\n",
    "    dataset = \"\"\n",
    "    for file in datafiles.get(edition):\n",
    "      with open(f\"./dataset/NF_{edition}{file}.txt\", \"r\", encoding='utf-8') as fr:\n",
    "        dataset += fr.read()\n",
    "        fr.close()\n",
    "        \n",
    "    b_tag_dict = build_b_tag_dataset(dataset, verbose=True)\n",
    "    print(f\"{edition} has {len(b_tag_dict):,} entries\")\n",
    "\n",
    "    with open(f\"./dataset/NF_{edition}_B.json\", \"w\") as b_json:\n",
    "      json.dump(b_tag_dict, b_json, indent=2, ensure_ascii=False)\n",
    "  del i, edition, dataset, file, fr, b_tag_dict, b_json\n",
    "# build_json_headword_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T01:03:04.526659Z",
     "iopub.status.busy": "2024-11-16T01:03:04.526523Z",
     "iopub.status.idle": "2024-11-16T01:05:47.982547Z",
     "shell.execute_reply": "2024-11-16T01:05:47.981956Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_headword_dataset():\n",
    "  dataset = []\n",
    "  for edition in ['E1', 'E2']:\n",
    "      with open(f\"./dataset/NF_{edition}_B.json\", \"r\", encoding='utf-8') as b_json:\n",
    "        dataset += json.load(b_json)\n",
    "        b_json.close()\n",
    "  return dataset\n",
    "\n",
    "def process_data(sentence, headword):\n",
    "    encoded_sentence = tokenizer(\n",
    "        sentence,\n",
    "        add_special_tokens=True, \n",
    "        padding='max_length',   \n",
    "        max_length=100,        \n",
    "        truncation=True,       \n",
    "        return_tensors='pt'  \n",
    "    )\n",
    "    encoded_headword = tokenizer(\n",
    "        headword,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        max_length=20,           \n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_sentence['input_ids'][0], encoded_headword['input_ids'][0]\n",
    "\n",
    "def extract_features_labels(dataset) -> Tuple[List, List]:\n",
    "    x = []\n",
    "    y = []\n",
    "    for entry in tqdm(dataset):\n",
    "      sentence, headword = process_data(entry[0], entry[1])\n",
    "      x.append(sentence)\n",
    "\n",
    "      min_len = min(len(sentence), len(headword))\n",
    "      headword_mask = np.where((sentence[:min_len] > 4) & (sentence[:min_len] == headword[:min_len]), 1, 0)\n",
    "      headword_mask = np.pad(headword_mask, (0, len(sentence) - min_len), 'constant')\n",
    "      \n",
    "      y.append(torch.tensor(headword_mask))\n",
    "    return torch.stack(x).to(device), torch.stack(y).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased\")\n",
    "dataset = load_headword_dataset()\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "test_set = dataset[:5000]\n",
    "dataset = dataset[5000:]\n",
    "\n",
    "# with open(f\"./dataset/NF_test_set_12.json\", \"w\") as test_json:\n",
    "#    json.dump(test_set, test_json, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:03<00:00, 1385.52it/s]\n",
      "100%|██████████| 303448/303448 [03:18<00:00, 1526.90it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = extract_features_labels(test_set) # <-- Use this to comapre different models. \n",
    "X, y = extract_features_labels(dataset)            # <-- Use this to train our model or fine-tune a model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: \n",
      "Bondesen, Ingvor, dansk författare, f. 1844, var 186485 skollärare på Fyn, blef därefter anställd i Köpenhamns skolväsen och 1892 skolinspektör vid en af skolorna därstädes. Han började 1877 under märket Henning Fox att skrifva historiska romaner från den äldre medeltiden, Styrismanden og hans brud ( 1877 ) och Kongsbrydens fostersön ( 1878 ). Senare följde berättelser med ämnen från adelsväldets tid, Rettergang og skriftegang\n",
      "\n",
      "Tokenized Sentence: \n",
      "[2, 17431, 436, 19, 1613, 8143, 19, 5908, 4567, 19, 13, 7, 21978, 49841, 19, 96, 42581, 5727, 16964, 4759, 68, 48349, 19, 21829, 49808, 3386, 7410, 31, 32754, 1005, 28343, 36, 35907, 1005, 17969, 252, 59, 4815, 13349, 256, 23055, 7, 371, 1649, 42456, 244, 19997, 15860, 13469, 48, 1147, 49808, 384, 8521, 15797, 146, 97, 2332, 21045, 19, 15307, 2894, 963, 5196, 699, 14503, 177, 42456, 171, 36, 7954, 9213, 389, 148, 11989, 21345, 177, 40999, 171, 7, 9475, 5622, 12631, 66, 7114, 146, 23410, 746, 1730, 353, 290, 19, 17187, 95, 19612, 5196, 4653, 24640, 49802, 3]\n",
      "\n",
      "Target Mask: \n",
      "[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Target Headword: \n",
      "Bondesen,\n"
     ]
    }
   ],
   "source": [
    "example_index = 1\n",
    "input = X[example_index]\n",
    "mask = y[example_index]\n",
    "headword = X[example_index][torch.flatten(torch.nonzero(y[example_index]))]\n",
    "print(f\"Input Sentence: \\n{tokenizer.decode(input, skip_special_tokens=True)}\\n\")\n",
    "print(f\"Tokenized Sentence: \\n{input.tolist()}\\n\")\n",
    "print(f\"Target Mask: \\n{mask.tolist()}\\n\")\n",
    "print(f\"Target Headword: \\n{tokenizer.decode(headword, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T01:05:47.986360Z",
     "iopub.status.busy": "2024-11-16T01:05:47.986218Z",
     "iopub.status.idle": "2024-11-16T01:05:48.146526Z",
     "shell.execute_reply": "2024-11-16T01:05:48.145965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50325\n",
      "Max input ID: 49968\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20) # <-- Use to train our model or fine-tune a model.\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Max input ID:\", torch.max(X_train).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T01:05:53.055633Z",
     "iopub.status.busy": "2024-11-16T01:05:53.055402Z",
     "iopub.status.idle": "2024-11-16T01:05:53.058296Z",
     "shell.execute_reply": "2024-11-16T01:05:53.057940Z"
    }
   },
   "outputs": [],
   "source": [
    "class HeadwordExtractor():\n",
    "  def __init__(self, saved_model):\n",
    "    self.embedding_dim = 128\n",
    "    self.hidden_dim = 128\n",
    "    self.batch_size = 32\n",
    "    self.num_epochs = 5\n",
    "    self.learning_rate = 0.001\n",
    "    self.saved_model = saved_model\n",
    "    \n",
    "    self.train_loader = DataLoader(TensorDataset(X_train.long(), y_train.long()), batch_size=self.batch_size, shuffle=True)\n",
    "    self.val_loader = DataLoader(TensorDataset(X_val.long(), y_val.long()), batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    self.model = self.EncoderLSTM(tokenizer.vocab_size, self.embedding_dim, self.hidden_dim, nbr_classes=2, num_layers=1,bidi_lstm=True).to(device)\n",
    "\n",
    "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    print(self.model)\n",
    "\n",
    "  class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, nbr_classes, num_layers=1, bidi_lstm=False):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=tokenizer.pad_token_id)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidi_lstm)\n",
    "        if not bidi_lstm:\n",
    "            self.fc = nn.Linear(hidden_dim, nbr_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(2*hidden_dim, nbr_classes)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)\n",
    "        encoder_out, _ = self.encoder(embeds)\n",
    "        encoder_out = nn.functional.relu(encoder_out)\n",
    "        logits = self.fc(encoder_out)\n",
    "        return logits\n",
    "\n",
    "  def train_extractor(self):\n",
    "    history=[]\n",
    "    for epoch in range(self.num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{self.num_epochs}:\")\n",
    "      \n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "        temp = 0\n",
    "        for input_batch, target_batch in tqdm(self.train_loader, desc = \"Training\"):\n",
    "            outputs = self.model(input_batch)\n",
    "\n",
    "            loss = self.criterion(outputs.view(-1,outputs.shape[-1]), target_batch.view(-1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_accuracy += (torch.sum(outputs.view(-1,outputs.shape[-1]).argmax(dim=1) == target_batch.view(-1))/target_batch.view(-1).shape[0]).item()\n",
    "        avg_train_loss = train_loss / len(self.train_loader)\n",
    "        avg_train_acc = train_accuracy / len(self.train_loader)\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_acc:.4f}\")\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            val_accuracy = 0\n",
    "            for input_batch, target_batch in tqdm(self.val_loader, desc = \"Validation\"):\n",
    "              outputs = self.model(input_batch)\n",
    "\n",
    "              loss = self.criterion(outputs.view(-1,outputs.shape[-1]), target_batch.view(-1))\n",
    "              val_loss += loss.item()\n",
    "              val_accuracy += (torch.sum(outputs.view(-1,outputs.shape[-1]).argmax(dim=1) == target_batch.view(-1))/target_batch.view(-1).shape[0]).item()\n",
    "        avg_val_loss = val_loss/len(self.val_loader)\n",
    "        avg_val_acc = val_accuracy / len(self.val_loader)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_acc:.4f}\\n\")\n",
    "\n",
    "        history.append((avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc))\n",
    "    self.__plot_metrics(history)\n",
    "\n",
    "  def __plot_metrics(self, history):\n",
    "    train_loss, train_acc, val_loss, val_acc = tuple(zip(*history))\n",
    "    epochs = range(1, len(history) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_acc, marker='o', linestyle='-', label='Train Accuracy')\n",
    "    plt.plot(epochs, val_acc, marker='o', linestyle='-', label='Validation Accuracy')\n",
    "    plt.title('Validation vs Train Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, train_loss, marker='o', linestyle='-', label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, marker='o', linestyle='-', label='Validation Loss')\n",
    "    plt.title('Validation vs Train Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "  def predict(self, encoded_input):\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "      output_mask = self.model(encoded_input).argmax(dim=-1).view(-1).cpu()\n",
    "    return output_mask\n",
    "  \n",
    "  def load_model(self):\n",
    "     self.model.load_state_dict(torch.load(self.saved_model, weights_only=True))\n",
    "     self.model.eval()\n",
    "  \n",
    "  def save_model(self):\n",
    "     torch.save(self.model.state_dict(), self.saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLSTM(\n",
      "  (embeddings): Embedding(50325, 128, padding_idx=0)\n",
      "  (encoder): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "SAVED_MODEL = \"headword_extractor.pth\"\n",
    "TRAIN_NEW_MODEL = False\n",
    "\n",
    "extractor = HeadwordExtractor(saved_model=SAVED_MODEL)\n",
    "\n",
    "if TRAIN_NEW_MODEL:\n",
    "  extractor.train_extractor()\n",
    "else:\n",
    "  extractor.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractor.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ipso facto, Lat., genom sjelfva sakförhållandet, i och genom sig sjelf.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ipso facto,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.decode(X_train[example_index], skip_special_tokens=True))\n",
    "tokenizer.decode(X_train[example_index][torch.flatten(torch.nonzero(y_train[example_index]))], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T04:02:51.484972Z",
     "iopub.status.busy": "2024-11-16T04:02:51.484820Z",
     "iopub.status.idle": "2024-11-16T04:02:51.586294Z",
     "shell.execute_reply": "2024-11-16T04:02:51.585934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted headword: Gustav Vasa,\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"Gustav Vasa, ursprungligen Gustav Eriksson,[2] enligt flera källor född 12 maj 1496, död 29 september 1560 på Tre Kronor i Stockholm.[3] var kung av Sverige 1523–1560 och riksföreståndare 1521–1523, under det pågående befrielsekriget. Hans makttillträde, inlett som ett uppror mot unionskungen Kristian II efter Stockholms blodbad, innebar slutet för\"\n",
    "encoded_input = tokenizer(input_sentence, return_tensors=\"pt\", padding = \"max_length\", max_length = 100, truncation = True)['input_ids'].to(device)\n",
    "\n",
    "output_mask = extractor.predict(encoded_input)\n",
    "headword = encoded_input.cpu().view(-1)[torch.flatten(torch.nonzero(output_mask))]\n",
    "print(\"Predicted headword:\", tokenizer.decode(headword, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[484594    822]\n",
      " [   606  13978]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    485416\n",
      "           1       0.94      0.96      0.95     14584\n",
      "\n",
      "    accuracy                           1.00    500000\n",
      "   macro avg       0.97      0.98      0.97    500000\n",
      "weighted avg       1.00      1.00      1.00    500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predictions = extractor.predict(X_test)\n",
    "y_test = y_test.view(-1).cpu()\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_predictions))\n",
    "print(classification_report(y_test, y_test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
