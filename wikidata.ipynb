{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40.662222222222, -74.209166666667)\n"
     ]
    }
   ],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "\n",
    "def fetch_coordinates(qid):\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": qid,\n",
    "        \"props\": \"claims\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract coordinates from property P625\n",
    "    claims = data.get(\"entities\", {}).get(qid, {}).get(\"claims\", {})\n",
    "    coordinate_claims = claims.get(\"P625\", [])\n",
    "    if coordinate_claims:\n",
    "        coordinates = coordinate_claims[0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "        lat, lon = coordinates[\"latitude\"], coordinates[\"longitude\"]\n",
    "        return lat, lon\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_wikidata_entity(qid):\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": qid,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch data: {response.status_code}\")\n",
    "\n",
    "# Load KB-BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased\")\n",
    "model = AutoModel.from_pretrained(\"KB/bert-base-swedish-cased\")\n",
    "\n",
    "# Define your SPARQL endpoint and query\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "headword = \"Elizabeth\"\n",
    "definition = \"stad i nordamerikanska staten New Jersey, vid Staten-Sound. 19 kilom. (2,5 mil) från New York, till hvilken den nästan kan anses som förstad. 20,832 innev. (1870). E. anlades 1665 och var 175557 New Jerseys hufvudstad.\"\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT ?item ?itemLabel ?description WHERE {{\n",
    "  ?item rdfs:label \"{headword}\"@sv.\n",
    "  ?item schema:description ?description.\n",
    "  FILTER(LANG(?description) = \"sv\").\n",
    "  SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],sv\". }}\n",
    "}}\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "sparql.setQuery(query)\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "# Extract results\n",
    "items = []\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    description = result[\"description\"][\"value\"]\n",
    "    if description.strip():  # Ensure description is not empty\n",
    "        items.append({\n",
    "            \"uri\": result[\"item\"][\"value\"],\n",
    "            \"label\": result[\"itemLabel\"][\"value\"],\n",
    "            \"description\": description\n",
    "        })\n",
    "\n",
    "# Function to compute sentence embeddings using KB-BERT\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use mean pooling over the token embeddings\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze(0).cpu()\n",
    "\n",
    "# Check if we have items to process\n",
    "if items:\n",
    "    # Encode definition and descriptions\n",
    "    definition_embedding = get_embedding(definition)\n",
    "    description_embeddings = torch.stack([get_embedding(item[\"description\"]) for item in items])\n",
    "\n",
    "    # Compute cosine similarity between definition and descriptions\n",
    "    cosine_scores = cosine_similarity(\n",
    "        definition_embedding.unsqueeze(0).numpy(),\n",
    "        description_embeddings.numpy()\n",
    "    ).flatten()\n",
    "\n",
    "    # Attach scores to items and sort by similarity\n",
    "    for i, score in enumerate(cosine_scores):\n",
    "        items[i][\"score\"] = score\n",
    "    items = sorted(items, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    # Display results\n",
    "    # for item in items:\n",
    "    #     print(f\"URI: {item['uri']}\")\n",
    "    #     print(f\"Label: {item['label']}\")\n",
    "    #     print(f\"Description: {item['description']}\")\n",
    "    #     print(f\"Score: {item['score']:.4f}\")\n",
    "    #     print()\n",
    "\n",
    "    # print(items[0])\n",
    "    parts = items[0]['uri'].split(\"/\")\n",
    "    # data = fetch_wikidata_entity(parts[len(parts)-1])\n",
    "    data = fetch_coordinates(parts[len(parts)-1])\n",
    "\n",
    "    # Print JSON response\n",
    "    print(data)\n",
    "else:\n",
    "    print(\"No valid items with descriptions found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv4",
   "language": "python",
   "name": "myenv4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
